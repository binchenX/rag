version: '3'
services:
  model:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      # assume you have installed pulled model locally already
      # OLLAMA_MODEL_DATA=/Users/binchen/.ollama/
      - ${OLLAMA_MODEL_DATA}:/root/.ollama
    tty: true
  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    ports:
      - 7654:7654
    environment:
      # when model running in local host
      # - MODEL_URL=http://host.docker.internal:11434
      - MODEL_URL=http://model:11434
      # init index only in the first run to index the external source
      # command it out in the following
      - INIT_INDEX=true
      # the index db dir (in container)
      # - INDEX_PERSIST_DIRECTORY=data/chromadb
      - EXTERNAL_DATA_URL=https://www.devicu.com/blog/
    volumes:
      # map local volume to avoid need to index it everytime
      - ./data:/app/data
    depends_on:
      - model
    # create web-ui service using docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080
    volumes:
      - ./data/webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://model:11434
